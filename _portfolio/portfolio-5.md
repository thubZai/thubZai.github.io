---
title: "CLIP Implementation"
excerpt: "Overview of Clip to understand what's going on under the hood!" 
collection: Projects 
---

[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/thubZ09/Clip_Implementation)


This repository implements a CLIP-like model that aligns image and text representations using PyTorch and Hugging Face Transformers. The model uses a Vision Transformer (ViT) as the image encoder and DistilRoBERTa as the text encoder. It is trained on the Flickr30k dataset.

Blog - **[ Understanding CLIP](https://thubzai.github.io/posts/2012/08/blog-post-4/)** 