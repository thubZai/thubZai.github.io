---
permalink: /
title: 
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---  

![multimodality](Untitled design.png){: .align-center width="1111px"}  

True intelligence emerges when open-ended multimodal systems that perceive and reason across modalities (vision, language & action), autonomously invent novel tasks, and learn to solve them, enabling generalization in unfamiliar, real-world settings. My current interests include: 

- *Vision‚ÄìLanguage integration, multimodal foundation models (VLMs, MLLMs, VLAMs)*
- *Reinforcement and Open-ended Learning that enable agents to autonomously explore, invent tasks, and bootstrap complexity* 
- *Multimodal perception (object-centric, 3D-aware perception for improving compositional/spatial reasoning and scene understanding)*

I believe progress here could meaningfully accelerate scientific discovery, embodied AI, and healthcare at scale. The impact could far surpass the current aspirations for AGI/ASI.  
Open to research collaborations and internships! 

üìåCheck out [Multimodal/VLMs Research Hub](https://github.com/thubZ09/vision-language-model-hub.git). I thought having a community-driven hub for multimodal researchers would be great. Contributions or suggestions are welcome!

‚úçÔ∏è I enjoy jotting down my thoughts and keeping an organized ‚Äúsecond brain‚Äù, unlike my primary one:) You can find some interesting stuff in the brain dump section above. 

Outside of work, you‚Äôll find me clicking random pictures, reading, trekking or playing and watching a variety of sports (football, cricket, MMA, & Esports).